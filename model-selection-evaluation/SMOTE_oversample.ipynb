{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Metrics/scoring\n",
    "\n",
    "**Classification binary and multi-class/label**\n",
    "\n",
    "Binary:\n",
    "- Accuracy (but not work well on imbalanced labels/classes)\n",
    "- Precision: Measures the proportion of true positives (TP) among the predicted positives. High precision means few false positives (predict positive but it's correctly negative = test covid but not really).$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "- Recall (Sensitivity): Measures the proportion of true positives (TP) among the actual positives.  High recall means fewer false negatives (predict negative but it's correctly positive = test non-covid but it's actually yes-covid, more seriously sometimes). $$\\text{Recall} = \\frac{TP}{TP + FN} $$\n",
    "- F1-Score: The harmonic mean of precision and recall. It balances precision and recall, especially when you want a single metric that considers both. $$\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "    - A high F1-score means both precision and recall are high.\n",
    "    - Useful when there is an imbalance between classes or when false positives and false negatives have different costs.\n",
    "\n",
    "- AUC-ROC\n",
    "\n",
    "\n",
    "Multi-class:\n",
    "- Macro-averaging\n",
    "- Micro-averaging\n",
    "- Weighted-averaging\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "# 4. Imbalanced class\n",
    "- Over/under-sampling\n",
    "- SMOTE (oversampling minority): [original paper]([View of SMOTE: Synthetic Minority Over-sampling Technique](https://www.jair.org/index.php/jair/article/view/10302/24590))\n",
    "\n",
    "https://towardsdatascience.com/smote-fdce2f605729\n",
    "\n",
    "#### **SMOTE**\n",
    "\n",
    "SMOTE is an algorithm that performs data augmentation by creating **synthetic data points** based on the original data points. SMOTE can be seen as an advanced version of oversampling, or as a specific algorithm for data augmentation. The advantage of SMOTE is that you are **not generating duplicates**, but rather creating synthetic data points that are **slightly different** from the original data points.\n",
    "\n",
    "> SMOTE is an improved alternative for oversampling\n",
    "\n",
    "The **SMOTE algorithm** works as follows:\n",
    "\n",
    "- You draw a random sample from the minority class.\n",
    "- For the observations in this sample, you will identify the k nearest neighbors.\n",
    "- You will then take one of those neighbors and identify the vector between the current data point and the selected neighbor.\n",
    "- You multiply the vector by a random number between 0 and 1.\n",
    "- To obtain the synthetic data point, you add this to the current data point.\n",
    "\n",
    "This operation is actually very much like **slightly moving the data point in the direction of its neighbor**. This way, you make sure that your synthetic data point is **not an exact copy** of an existing data point while making sure that it is **also not too different** from the known observations in your minority class.\n",
    "##### SMOTE influences precision vs. recall\n",
    "\n",
    "In the previously presented mountain sports example, we have looked at the overall accuracy of the model. Accuracy measures the percentages of predictions that you got right. In classification problems, we generally want to go a bit further than that and take into account **predictive performance for each class**.\n",
    "\n",
    "In binary classification, the **confusion matrix** is a machine learning metric that shows the number of:\n",
    "- _true positives (the model correctly predicted true)_\n",
    "- _false positives (the model incorrectly predicted true)_\n",
    "- _true negatives_ _(the model correctly predicted false)_\n",
    "- _false negatives (the model incorrectly predicted false)_\n",
    "\n",
    "In this context, we also talk about **precision vs. recall**. Precision means how well a model succeeds in identifying **ONLY positive cases**. Recall means how well a model succeeds in identifying **ALL the positive cases within the data**.\n",
    "\n",
    "True positives and true negatives are both correct predictions: having many of those is the ideal situation. False positives and false negatives are both wrong predictions: having little of them is the ideal case as well. Yet in many cases, **we may prefer having false positives rather than having false negatives**.\n",
    "\n",
    "When machine learning is used for automating business processes, false negatives (positives that are predicted as negative) will not show up anywhere and will probably never be detected, whereas false positives (negatives that are wrongly predicted as positive) will generally be filtered out quite easily in later manual checks that many businesses have in place.\n",
    "\n",
    "> In many business cases, false positives are less problematic than false negatives.\n",
    "\n",
    "An obvious example would be **testing for the coronavirus**. Imagine that sick people take a test and they obtain a false negative: they will go out and infect other people. On the other hand, if they are false positive they will be obliged to stay home: not ideal, but at least they do not form a public health hazard.\n",
    "\n",
    "When we have a strong class imbalance, we have very few cases in one class, resulting in the model hardly ever predicting that class. **Using SMOTE we can tweak the model to reduce false negatives, at the cost of increasing false positives.** The result of using SMOTE is generally an **increase in recall**, at the cost of **lower precision**. This means that we will add **more predictions of the minority class**: some of them correct (increasing recall), but some of them wrong (decreasing precision).\n",
    "\n",
    "> SMOTE increases recall at the cost of lower precision\n",
    "\n",
    "For example, a model that predicts buyers all the time will be good in terms of recall, as it did identify all the positive cases. Yet it will be bad in terms of precision. The overall model accuracy may also decrease, but this is not a problem: **accuracy should not be used as a metric in case of imbalanced data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.548814</td>\n",
       "      <td>0.264556</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.715189</td>\n",
       "      <td>0.774234</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.602763</td>\n",
       "      <td>0.456150</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.544883</td>\n",
       "      <td>0.568434</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.423655</td>\n",
       "      <td>0.018790</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.645894</td>\n",
       "      <td>0.617635</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.437587</td>\n",
       "      <td>0.612096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.891773</td>\n",
       "      <td>0.616934</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.963663</td>\n",
       "      <td>0.943748</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.383442</td>\n",
       "      <td>0.681820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.791725</td>\n",
       "      <td>0.359508</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.528895</td>\n",
       "      <td>0.437032</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.568045</td>\n",
       "      <td>0.697631</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.925597</td>\n",
       "      <td>0.060225</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.071036</td>\n",
       "      <td>0.666767</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.087129</td>\n",
       "      <td>0.670638</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.020218</td>\n",
       "      <td>0.210383</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.832620</td>\n",
       "      <td>0.128926</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.778157</td>\n",
       "      <td>0.315428</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.870012</td>\n",
       "      <td>0.363711</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.978618</td>\n",
       "      <td>0.570197</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.799159</td>\n",
       "      <td>0.438602</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.461479</td>\n",
       "      <td>0.988374</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.780529</td>\n",
       "      <td>0.102045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.118274</td>\n",
       "      <td>0.208877</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.639921</td>\n",
       "      <td>0.161310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.143353</td>\n",
       "      <td>0.653108</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.944669</td>\n",
       "      <td>0.253292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.521848</td>\n",
       "      <td>0.466311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.414662</td>\n",
       "      <td>0.244426</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1    2\n",
       "0   0.548814  0.264556  0.0\n",
       "1   0.715189  0.774234  0.0\n",
       "2   0.602763  0.456150  0.0\n",
       "3   0.544883  0.568434  0.0\n",
       "4   0.423655  0.018790  0.0\n",
       "5   0.645894  0.617635  0.0\n",
       "6   0.437587  0.612096  0.0\n",
       "7   0.891773  0.616934  0.0\n",
       "8   0.963663  0.943748  0.0\n",
       "9   0.383442  0.681820  0.0\n",
       "10  0.791725  0.359508  0.0\n",
       "11  0.528895  0.437032  1.0\n",
       "12  0.568045  0.697631  0.0\n",
       "13  0.925597  0.060225  1.0\n",
       "14  0.071036  0.666767  1.0\n",
       "15  0.087129  0.670638  0.0\n",
       "16  0.020218  0.210383  0.0\n",
       "17  0.832620  0.128926  0.0\n",
       "18  0.778157  0.315428  0.0\n",
       "19  0.870012  0.363711  0.0\n",
       "20  0.978618  0.570197  0.0\n",
       "21  0.799159  0.438602  0.0\n",
       "22  0.461479  0.988374  0.0\n",
       "23  0.780529  0.102045  0.0\n",
       "24  0.118274  0.208877  0.0\n",
       "25  0.639921  0.161310  0.0\n",
       "26  0.143353  0.653108  0.0\n",
       "27  0.944669  0.253292  0.0\n",
       "28  0.521848  0.466311  0.0\n",
       "29  0.414662  0.244426  0.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "f1 = np.random.uniform(0, 1, size=30)\n",
    "f2 = np.random.uniform(0, 1, size=30)\n",
    "y = np.array([0] * round(0.89 * 30) + [1] * round(0.11 * 30))\n",
    "np.random.shuffle(y)\n",
    "\n",
    "pd.DataFrame(np.c_[f1, f2, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.548814</td>\n",
       "      <td>0.264556</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.715189</td>\n",
       "      <td>0.774234</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.602763</td>\n",
       "      <td>0.456150</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.544883</td>\n",
       "      <td>0.568434</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.423655</td>\n",
       "      <td>0.018790</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.645894</td>\n",
       "      <td>0.617635</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.437587</td>\n",
       "      <td>0.612096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.891773</td>\n",
       "      <td>0.616934</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.963663</td>\n",
       "      <td>0.943748</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.383442</td>\n",
       "      <td>0.681820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.791725</td>\n",
       "      <td>0.359508</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.528895</td>\n",
       "      <td>0.437032</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.568045</td>\n",
       "      <td>0.697631</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.925597</td>\n",
       "      <td>0.060225</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.071036</td>\n",
       "      <td>0.666767</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.087129</td>\n",
       "      <td>0.670638</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.020218</td>\n",
       "      <td>0.210383</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.832620</td>\n",
       "      <td>0.128926</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.778157</td>\n",
       "      <td>0.315428</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.870012</td>\n",
       "      <td>0.363711</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.978618</td>\n",
       "      <td>0.570197</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.799159</td>\n",
       "      <td>0.438602</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.461479</td>\n",
       "      <td>0.988374</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.780529</td>\n",
       "      <td>0.102045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.118274</td>\n",
       "      <td>0.208877</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.639921</td>\n",
       "      <td>0.161310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.143353</td>\n",
       "      <td>0.653108</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.944669</td>\n",
       "      <td>0.253292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.521848</td>\n",
       "      <td>0.466311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.414662</td>\n",
       "      <td>0.244426</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.386346</td>\n",
       "      <td>0.508557</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.260710</td>\n",
       "      <td>0.571596</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.329813</td>\n",
       "      <td>0.536923</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.407384</td>\n",
       "      <td>0.498001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.114048</td>\n",
       "      <td>0.645185</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.892626</td>\n",
       "      <td>0.091543</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1    2\n",
       "0   0.548814  0.264556  0.0\n",
       "1   0.715189  0.774234  0.0\n",
       "2   0.602763  0.456150  0.0\n",
       "3   0.544883  0.568434  0.0\n",
       "4   0.423655  0.018790  0.0\n",
       "5   0.645894  0.617635  0.0\n",
       "6   0.437587  0.612096  0.0\n",
       "7   0.891773  0.616934  0.0\n",
       "8   0.963663  0.943748  0.0\n",
       "9   0.383442  0.681820  0.0\n",
       "10  0.791725  0.359508  0.0\n",
       "11  0.528895  0.437032  1.0\n",
       "12  0.568045  0.697631  0.0\n",
       "13  0.925597  0.060225  1.0\n",
       "14  0.071036  0.666767  1.0\n",
       "15  0.087129  0.670638  0.0\n",
       "16  0.020218  0.210383  0.0\n",
       "17  0.832620  0.128926  0.0\n",
       "18  0.778157  0.315428  0.0\n",
       "19  0.870012  0.363711  0.0\n",
       "20  0.978618  0.570197  0.0\n",
       "21  0.799159  0.438602  0.0\n",
       "22  0.461479  0.988374  0.0\n",
       "23  0.780529  0.102045  0.0\n",
       "24  0.118274  0.208877  0.0\n",
       "25  0.639921  0.161310  0.0\n",
       "26  0.143353  0.653108  0.0\n",
       "27  0.944669  0.253292  0.0\n",
       "28  0.521848  0.466311  0.0\n",
       "29  0.414662  0.244426  0.0\n",
       "30  0.386346  0.508557  1.0\n",
       "31  0.260710  0.571596  1.0\n",
       "32  0.329813  0.536923  1.0\n",
       "33  0.407384  0.498001  1.0\n",
       "34  0.114048  0.645185  1.0\n",
       "35  0.892626  0.091543  1.0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def custom_smote(X, y, minority_class=1, N=100, k=5):\n",
    "    \"\"\"\n",
    "    SMOTE implementation for generating synthetic samples.\n",
    "    \n",
    "    Params:\n",
    "        X: Feature matrix (numpy array).\n",
    "        y: Target array.\n",
    "        minority_class: Label of the minority class (binary so this is assumed to be 1)\n",
    "\n",
    "        N: Percentage of new synthetic samples (in % of the minority class size).\n",
    "        \n",
    "        k: Number of nearest neighbors to consider.\n",
    "\n",
    "    Returns:\n",
    "        X_resampled, y_resampled: Resampled feature matrix and target array.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: extract minority samples\n",
    "    X_minority = X[y == minority_class]\n",
    "    n_minority = X_minority.shape[0]\n",
    "    \n",
    "    n_synthetic = int(N / 100 * n_minority)  # number of synthetic samples to generate (conventional way)\n",
    "\n",
    "    # Step 2: fit KNN\n",
    "    knn = NearestNeighbors(n_neighbors=k).fit(X_minority)\n",
    "\n",
    "    # Step 3: Generate synthetic samples\n",
    "    synthetic_samples = []\n",
    "\n",
    "    for _ in range(n_synthetic):\n",
    "\n",
    "        # Randomly choose a minority sample\n",
    "        i = np.random.randint(0, n_minority)\n",
    "        x_min = X_minority[i]\n",
    "        \n",
    "        # Find k-nearest neighbors\n",
    "        neighbors = knn.kneighbors(x_min.reshape(1, -1), return_distance=False)[0]\n",
    "        \n",
    "        # Randomly pick one neighbor\n",
    "        neighbor_idx = np.random.choice(neighbors[1:])  # Avoid self-pairing\n",
    "        x_neighbor = X_minority[neighbor_idx]\n",
    "        \n",
    "        # Generate synthetic sample by interpolation\n",
    "        lam = np.random.uniform(0, 1)\n",
    "        synthetic_sample = x_min + lam * (x_neighbor - x_min)\n",
    "        synthetic_samples.append(synthetic_sample)\n",
    "    \n",
    "    # Step 4: Concatenate the original and synthetic samples\n",
    "    X_synthetic = np.array(synthetic_samples)\n",
    "    y_synthetic = np.array([minority_class] * n_synthetic)\n",
    "\n",
    "    X_resampled = np.vstack((X, X_synthetic))\n",
    "    y_resampled = np.hstack((y, y_synthetic))\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "X_train = np.c_[f1, f2]\n",
    "y_train = y\n",
    "\n",
    "X_resampled, y_resampled = custom_smote(X_train, y_train, minority_class=1, N=200, k=2)\n",
    "\n",
    "pd.DataFrame(np.c_[X_resampled, y_resampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why randomly choose a minority sample but not go over each minority sample?\n",
    "\n",
    "-> Avoid overfitting: If systematically generate synthetic samples for every minority sample, might produce many points that are clustered closely around the original minority points, potentially overfitting to the minority class distribution. By randomly sampling, increase the diversity of synthetic points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((336, 7),\n",
       " (336,),\n",
       " (array([-1,  1], dtype=int64), array([301,  35], dtype=int64)))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.datasets import fetch_datasets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "data = fetch_datasets()['ecoli']\n",
    "data.data.shape, data.target.shape, np.unique(data.target, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([-1,  1], dtype=int64), array([240,  28], dtype=int64)),\n",
       " (array([-1,  1], dtype=int64), array([240,  84], dtype=int64)))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(data.data, data.target, test_size=0.2, random_state=0, stratify=data.target)\n",
    "\n",
    "X_train_resampled, y_train_resampled = custom_smote(X_train, y_train, N=200, k=5)\n",
    "\n",
    "np.unique(y_train, return_counts=True), np.unique(y_train_resampled, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9264705882352942,\n",
       " 0.6666666666666666,\n",
       " 0.5714285714285714,\n",
       " array([[59,  2],\n",
       "        [ 3,  4]], dtype=int64))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not resampled\n",
    "\n",
    "rf1 = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
    "y_pred = rf1.predict(X_valid)\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "precision = precision_score(y_valid, y_pred, zero_division=1)\n",
    "recall = recall_score(y_valid, y_pred, zero_division=1)\n",
    "conf_matrix = confusion_matrix(y_valid, y_pred)\n",
    "accuracy, precision, recall, conf_matrix \n",
    "\n",
    "# accuracy is \"high\"! very misleading\n",
    "# recall is terribly low\n",
    "# TN | FP\n",
    "# FN | TP\n",
    "\n",
    "# https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_1_5_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-1-5-0-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9264705882352942,\n",
       " 0.625,\n",
       " 0.7142857142857143,\n",
       " array([[58,  3],\n",
       "        [ 2,  5]], dtype=int64))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resampled\n",
    "\n",
    "rf2 = RandomForestClassifier(random_state=0).fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = rf2.predict(X_valid)\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "precision = precision_score(y_valid, y_pred, zero_division=1)\n",
    "recall = recall_score(y_valid, y_pred, zero_division=1)\n",
    "conf_matrix = confusion_matrix(y_valid, y_pred)\n",
    "accuracy, precision, recall, conf_matrix \n",
    "\n",
    "# recall is higher!\n",
    "# TN | FP\n",
    "# FN | TP\n",
    "# note that FN decreases (recall higher) and FP increases (precision lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1], dtype=int64), array([240, 108], dtype=int64))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use SMOTE from imblearn\n",
    "\n",
    "# tuning\n",
    "ratio = 0.45\n",
    "# ratio = 0.5\n",
    "\n",
    "smote = SMOTE(\n",
    "    random_state=0, \n",
    "    sampling_strategy=ratio,  # sampling_strategy = ratio between minority and majority\n",
    "    k_neighbors=5\n",
    ")\n",
    "X_train_resampled_SMOTE, y_train_resampled_SMOTE = smote.fit_resample(X_train, y_train)\n",
    "np.unique(y_train_resampled_SMOTE, return_counts=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9264705882352942,\n",
       " 0.625,\n",
       " 0.7142857142857143,\n",
       " array([[58,  3],\n",
       "        [ 2,  5]], dtype=int64))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf3 = RandomForestClassifier(random_state=0).fit(X_train_resampled_SMOTE, y_train_resampled_SMOTE)\n",
    "y_pred = rf3.predict(X_valid)\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "precision = precision_score(y_valid, y_pred, zero_division=1)\n",
    "recall = recall_score(y_valid, y_pred, zero_division=1)\n",
    "conf_matrix = confusion_matrix(y_valid, y_pred)\n",
    "accuracy, precision, recall, conf_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
